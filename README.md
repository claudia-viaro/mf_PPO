# mf_PPO


train_1.py 
is a basic PPO
one episode = one trajectory, as many steps as transitions are not yet "Done"
first 5 episode of random actions to fill in the buffer


train_2.py

PPO with GP addition
one episode = one trajectory, as many steps as transitions are not yet "Done"
first 5 episode of random actions to fill in the buffer