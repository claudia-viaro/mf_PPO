07:31:11
Namespace(gamma=0.99, epsilon=0.1, c_learning_rate=0.001, a_learning_rate=0.001, tau=0.97, n_episodes=30, batch_size=10, log_steps=10, save_steps=20, n_hidden=64, actor_save_path='saved/actor_ppo.pth', critic_save_path='saved/critic_ppo.pth', checkpoint_interval=100000.0, out='/tmp/ppo/models/', reset_dir=False, test_interval=10, domain_name='model', log_dir='train_2', config_name='model_update', strategy='information', seed=0, value_loss_coef=0.5, entropy_coef=0.01, buffer_capacity=1000, seed_episodes=5, task_name='run', all_episodes=10, action_noise_var=0.3, chunk_length=7, collect_interval=7, alpha=0.99, max_grad_norm=0.5)
step [1] Reward 0.71, LR Rewards 0.33
step [2] Reward 0.34, LR Rewards 0.27
step [3] Reward 0.37, LR Rewards 0.33
step [4] Reward 0.26, LR Rewards 0.37
step [5] Reward 0.26, LR Rewards 0.31
episode [1/10] is collected. Mean Rewards 0.32, Mean LR Rewards 0.27 over 6 transitions, it took 86.97 min
step [1] Reward 0.58, LR Rewards 0.92
step [2] Reward 0.33, LR Rewards 0.29
step [3] Reward 0.31, LR Rewards 0.25
step [4] Reward 0.33, LR Rewards 0.38
step [5] Reward 0.25, LR Rewards 0.32
episode [2/10] is collected. Mean Rewards 0.30, Mean LR Rewards 0.36 over 6 transitions, it took 84.65 min
step [1] Reward 0.63, LR Rewards 0.59
step [2] Reward 0.30, LR Rewards 0.81
step [3] Reward 0.42, LR Rewards 0.28